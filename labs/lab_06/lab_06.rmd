---
title: "lab_06"
author: "derek willis"
date: "2022-03-14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load libraries and establish settings

```{r}
# Turn off scientific notation
options(scipen=999)

# load your libraries
library(tidyverse)
library(janitor)
library(lubridate)
```

## Getting Started

This week we'll be combining and joining data to make it more useful and to ask some questions about it. We'll use some of our 911 overdose call data to do this. The first thing we want to do is to combine multiple counties' data into a single dataframe so that we can ask some questions. First, let's combine data from Cecil, Carroll and Allegany counties into a new dataframe.

*Before* you combine them you'll need to clean up some of the column names to make the data easier to work with - make sure you have loaded the library to do that. You also need to make sure that each column has the same name and datatype (you can check the datatypes using `glimpse`). If any of them does not, you need to fix that.

```{r}
# load and combine the call data from those three counties
allegany_911 <- read_csv("data/allegany_911.csv")
cecil_911 <- read_csv("data/cecil_911.csv") 
carroll_911 <- read_csv("data/carroll_911.csv") |> clean_names()

calls_combined <- bind_rows(list(allegany_911, cecil_911, carroll_911))

```

Then we'll join some demographic data with Baltimore City 911 calls and ask some questions of that. For that, you'll need to load the Baltimore City 911 data and the ZCTA demographic data, and join them based on the zip code. Remember, the zip codes in both dataframes must be the same datatype (and should be characters). You should start with your Baltimore 911 dataframe in creating your join.

```{r}
# load and join the Baltimore-related data
baltimore_911 <- read.csv("data/baltimore_911.csv")
maryland_zcta <- read.csv("data/maryland_zcta.csv")

maryland_zcta <- maryland_zcta |> mutate(ZCTA5N = as.character(ZCTA5N))
baltimore_911 <- baltimore_911 |> mutate(zip_code = as.character (zip_code))

baltimore_calls_demographics <- maryland_zcta |> left_join(baltimore_911, join_by(ZCTA5N==zip_code))
```

## Answer questions

### Q1. Let's start with our data from Allegany, Cecil and Carroll counties. Write code to generate a dataframe that shows the total number of calls for each county. What's the order of counties from most calls to least?

**A1. - Carroll County has the most calls with 448, then Cecil with 438, then Allegany with 418.**

```{r}
calls_combined |>
group_by(county) |>
  summarise(
    
    total_calls_per_county = n()
  ) |>
arrange(desc(total_calls_per_county))

```

### Q2. In your combined dataframe that you used in A1, add a column for the month of each call, then write code to show the total number of calls per county and month. Are there any outliers or noteworthy results? Describe the general pattern of the results.

Then calculate the most calls per county and location. Which location had the most calls - where and what is that location?

**A2. August and September seem to have more calls than others in Cecil and Allegany county. Then, when we broadened the search to add location, 291 E Pulaski Hwy in Elkton, MD (in Cecil) has the most calls with 4 total calls.**

```{r}
calls_combined |>
  mutate(month = month(date)) |>
group_by(county, month, location) |>
  summarise(
    
    total_calls_per_county = n()
  ) |>
arrange(desc(total_calls_per_county))
```

### Q3. Let's turn to the Baltimore City data. You're interested in exploring calls in zip codes with a high percentage of children under 18. Write code to isolate calls in zip codes where the percentage of under 18 population is at least 75% and then show the zip code and population and how many calls occurred in each of those zip codes.

Where are those zip codes in the city? **21217 is Central Baltimore, 21223 is South Baltimore, 21202 is East Baltimore, 21213 is Northeast Baltimore**

Q3b. Add a column to your code that calculates the number of calls per 1,000 people for those zip codes. To calculate a per capita rate, you might find this short guide handy: [https://observablehq.com/\@palewire/per-capita-calculator](https://observablehq.com/@palewire/per-capita-calculator){.uri}.

Which zip code has the highest rate of calls per 1,000 people? Find the neighborhoods in that zip code that are listed in the data - you can use group_by or distinct to do this. What are some of those neighborhoods? **North Hartford Road and Hamilton Hills.**

A3.

```{r}
baltimore_calls_demographics |>
  filter(PCT_UNDER_18 >= 75) |>
  group_by(ZCTA5N) |>                       
  summarise(population = POPULATION[1],   
            total_calls = n()) |>           
  arrange(desc(total_calls)) |>           
  select(ZCTA5N, population, total_calls)

baltimore_calls_demographics <- baltimore_calls_demographics |>
  group_by(ZCTA5N) |>
  mutate(
    total_calls = n(),
    calls_per_1000 = (total_calls / sum(POPULATION)) * 1000
  )

highest_rate_calls <- baltimore_calls_demographics |>
  group_by(ZCTA5N, neighborhood) |>
  summarise(
    total_calls = n(),
    population = sum(POPULATION),
    calls_per_1000 = (total_calls / population) * 1000
  ) |>
  arrange(desc(calls_per_1000))

```

### Q4. Choose your own adventure: I want you to come up with a question you could ask of the Baltimore City data that combines both the calls and the demographic information. You must use both demographics and the overdose call data in your work.

**A4: What zip code has the highest percentage of Black residents, and what neighborhoods are included in that zip code, along with the total number of overdose calls? 21216 has the highest percentage of Black residents with 96.1% and the most total overdose calls with 2016. The neighborhoods included in this zipcode is Bridgeview/Greenlawn.**

**NOTE:** I know that before we used a function to remove N/A from teh dataset, but I could not find it in the previous pre-labs. So I asked Chat GPT: "when coding in R, how do you make sure your result disregards NA in a dataset?" and it told me:

"In R, to ensure your results disregard `NA` values in a dataset, you can use the argument `na.rm = TRUE` in many functions. This will remove or ignore missing values when performing calculations or summary operations. Here are some common examples:

1.  **For sums or means:**

    ``` r
    sum(data$column, na.rm = TRUE)
    mean(data$column, na.rm = TRUE)
    ```

2.  **For filtering out NAs in a dataset:** You can use `na.omit()` to remove rows with any `NA` values.

    ``` r
    clean_data <- na.omit(data)
    ```

3.  **For removing NAs in a specific column:** You can filter rows that do not have `NA` in a particular column.

    ``` r
    clean_data <- data[!is.na(data$column), ]
    ```

By using these methods, you ensure that `NA` values are disregarded in your calculations or data analysis."

```{r}
baltimore_calls_demographics |>
  mutate(pct_black = PCT_BLACK_NON_HISPANIC) |>
  group_by(ZCTA5N, neighborhood) |>
  summarise(total_overdose_calls = sum(total_calls, na.rm = TRUE), 
            pct_black = mean(pct_black, na.rm = TRUE)) |>
  arrange(desc(pct_black)) |>
  select(ZCTA5N, neighborhood, pct_black, total_overdose_calls)
```
